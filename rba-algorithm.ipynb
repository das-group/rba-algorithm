{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Basic Algorithm for Risk-Based Authentication\n",
    "\n",
    "&copy; 2022 Stephan Wiefling / [Data and Application Security Group](https://das.h-brs.de)\n",
    "\n",
    "The code in this repository is licensed under the MIT License. See the [GitHub repository](https://github.com/das-group/rba-algorithm) for details.\n",
    "\n",
    "## Prerequisites\n",
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import copy\n",
    "from functools import partial\n",
    "import os\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define additional functions that enhance pandas's functionality\n",
    "\n",
    "We add custom functions to pandas, which can be very helpful when working with large datasets and high-performance computing clusters.\n",
    "\n",
    "These can be called with:\n",
    "\n",
    "- [var] = nested_dict()\n",
    "- [pandas DataFrame].before_timestamp(...)\n",
    "- [pandas DataFrame].between_timestamp(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"`var = nested_dict()` creates a `dict` which auto-creates empty entries as list elements\"\"\"\n",
    "nested_dict = lambda: defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __dataframe_values_before_timestamp(df, timestamp):\n",
    "    \"\"\"Query logs where time < timestamp (one element before timestamp)\n",
    "    index of logs needs to be sorted and of type DateTimeIndex for best performance.\n",
    "\n",
    "    INFO: timestamp must exist in the DataFrame, otherwise one element is lost.\n",
    "    This is not changed due to optimizing performance in the RBA algorithm.\n",
    "\n",
    "    timestamp -- timestamp (numpy.datetime64 or Date string)\"\"\"\n",
    "\n",
    "    return df.loc[:timestamp][:-1]\n",
    "\n",
    "pd.core.frame.DataFrame.before_timestamp = __dataframe_values_before_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __dataframe_values_between_timestamp(df, timestamp_from, timestamp_to):\n",
    "    \"\"\"Query logs where timestamp_from > time < timestamp_to (one element before timestamp)\n",
    "    index of logs needs to be sorted and of type DateTimeIndex for best performance.\n",
    "\n",
    "    INFO: timestamp_to must exist in the DataFrame, otherwise one element is lost.\n",
    "    This is not changed due to optimizing performance in the RBA algorithm.\n",
    "\n",
    "    timestamp -- timestamp (numpy.datetime64 or Date string)\"\"\"\n",
    "    return df.loc[timestamp_from:timestamp_to][:-1]\n",
    "\n",
    "pd.core.frame.DataFrame.between_timestamp = __dataframe_values_between_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashTable():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__hash_table = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubfeatureOccurenceHashTable(HashTable):\n",
    "\n",
    "    def __init__(self, df, column_name, subfeature, subfeatures):\n",
    "        self.__hash_table = {}\n",
    "        self.__subfeature_table = {}\n",
    "\n",
    "        # Column name\n",
    "        self.__column_name = column_name\n",
    "\n",
    "        # Subfeature belonging to this column name\n",
    "        self.__subfeature = subfeature\n",
    "\n",
    "        # List of all subfeatures belonging to this column name\n",
    "        self.__subfeatures = subfeatures[column_name]\n",
    "    \n",
    "        # Create subfeature hash table\n",
    "        self.__hash_table = df.groupby([self.__column_name, subfeature], dropna=False).apply(len)\n",
    "\n",
    "        group = [self.__column_name, self.__subfeature]\n",
    "\n",
    "        # Get position in list\n",
    "        pos = self.__subfeatures.index(self.__subfeature)\n",
    "\n",
    "        # Create list of subfeatures to use\n",
    "        self.__subfeatures_to_use = self.__subfeatures[pos:]\n",
    "\n",
    "        # Create subfeature table containing only the unique velues (We add true here to save space)\n",
    "        self.__subfeature_table = df.groupby(self.__subfeatures_to_use, dropna=False).apply(lambda x: True)\n",
    "\n",
    "\n",
    "    def get(self, subfeature, subfeature_value):\n",
    "        \"\"\"Get the occurence of a given column and subfeature value from the hash table\n",
    "    \n",
    "        column_name -- Column in hash table (\"feature name\")\n",
    "        subfeature -- Subfeature in hash table\n",
    "        subfeature_value -- Value of subfeature that we want to check the occurence for\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract occurence from hash table\n",
    "            return self.__hash_table.loc[subfeature_value]\n",
    "        except KeyError:\n",
    "            # Value not found, so the occurence is zero\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def increase(self, log):\n",
    "        \"\"\"Increase the occurences of subfeatures based on the log\"\"\"        \n",
    "        try: \n",
    "            column_name_value = getattr(log, self.__column_name)\n",
    "\n",
    "            # Get subfeature value from log\n",
    "            subfeature_value = getattr(log, self.__subfeature)\n",
    "        except AttributeError:\n",
    "            print(\"ERROR: Subfeature or column name does not exist in log\")\n",
    "\n",
    "        # Get value from hash table\n",
    "        try:\n",
    "            # Increase value\n",
    "            self.__hash_table.loc[(subfeature_value, column_name_value)] = self.__hash_table.loc[(subfeature_value, column_name_value)] + 1\n",
    "        except (KeyError, TypeError):\n",
    "            #  Key does not exist yet, so we start with 1\n",
    "            self.__hash_table.loc[(subfeature_value, column_name_value)] = 1\n",
    "\n",
    "            # Sort index\n",
    "            self.__hash_table.sort_index(inplace=True)\n",
    "\n",
    "        try:\n",
    "            for subfeature in __subfeature_table.keys():\n",
    "                # Get values of group hierarchy, start with first value in hierarchy\n",
    "                group_values = [getattr(log, self.__column_name)]\n",
    "                for pre_subfeature in self.__subfeatures:\n",
    "                    # Append value for the pre-subfeature and subfeature\n",
    "                    group_values.append(getattr(log, pre_subfeature))\n",
    "\n",
    "                    if pre_subfeature == self.__subfeature:\n",
    "                        # Abort when we reached this subfeature\n",
    "                        break\n",
    "\n",
    "                # Convert to tuple to address the values in table\n",
    "                group_values = tuple(group_values)\n",
    "\n",
    "                # Set value to true (adds new entry if not present)\n",
    "                self.__subfeature_table[subfeature].loc[group_values] = True\n",
    "\n",
    "                # Sort the table\n",
    "                self.__subfeature_table[subfeature].sort_index(inplace=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def len(self, subfeature_value):\n",
    "        \"\"\"Return the total number of values per subfeature value\"\"\"\n",
    "        return self.get(self.__subfeature, subfeature_value).sum()\n",
    "\n",
    "    def len_unique(self, next_subfeature):\n",
    "        \"\"\"Returns the number of unique values for a given subfeature\"\"\"\n",
    "        if next_subfeature == self.__subfeature:\n",
    "            # Calculate from hash table\n",
    "            return len(self.__hash_table)\n",
    "        else:\n",
    "            # Take value from subfeature table\n",
    "            return len(self.__subfeature_table[next_subfeature])\n",
    "\n",
    "    def get_hash_table(self, subfeature_value):\n",
    "        return self.__hash_table.loc[subfeature_value]\n",
    "\n",
    "    def get_full_hash_table(self):\n",
    "        return self.__hash_table\n",
    "\n",
    "\n",
    "    def get_full_subfeature_table(self):\n",
    "        return self.__subfeature_table\n",
    "\n",
    "    def get_column_name(self):\n",
    "        return self.__column_name\n",
    "\n",
    "    def get_subfeatures(self):\n",
    "        \"\"\"Returns the list of subfeatures\"\"\"\n",
    "        return self.__subfeatures\n",
    "\n",
    "    def get_subfeatures_to_use(self):\n",
    "        \"\"\"Returns the list of subfeatures to use\"\"\"\n",
    "        return self.__subfeatures_to_use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OccurenceHashTable(HashTable):\n",
    "    \"\"\"Class for hash tables containing the occurences of column values inside a DataFrame\"\"\"\n",
    "\n",
    "    def __init__(self, df, column_names=[], subfeatures={\n",
    "        \"requestXRealIP\": [\"ipASN\", \"ipCountry\"],\n",
    "        \"trackingdatauserAgent\": [\"browser_name_version\", \"os_name_version\", \"device_type\"]\n",
    "        }):\n",
    "        \"\"\"Creates the hash table\n",
    "\n",
    "        df -- pandas DataFrame\n",
    "        column_name -- List of column names in df (default: Select all columns)\n",
    "        \"\"\"\n",
    "        if len(column_names) == 0:\n",
    "            # Take column names from all columns of DataFrame\n",
    "            column_names = df.columns\n",
    "\n",
    "        self.__hash_table = {}\n",
    "\n",
    "        for column_name in column_names:\n",
    "            self.__hash_table[column_name] = pd.DataFrame(df.groupby(column_name, dropna=False).apply(len))[0]\n",
    "\n",
    "            # If column has subfeatures\n",
    "            if column_name in subfeatures.keys():\n",
    "                for subfeature in subfeatures[column_name]:\n",
    "                    # Create subfeature hash tables\n",
    "                    self.__hash_table[(column_name, subfeature)] = SubfeatureOccurenceHashTable(df, column_name, subfeature, subfeatures)\n",
    "                    \n",
    "        # Total length of DataFrame\n",
    "        self.__length = len(df)\n",
    "\n",
    "        # Store the list of subfeatures\n",
    "        self.__subfeatures = subfeatures\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.__hash_table)\n",
    "        \n",
    "    def get(self, column_name, value):\n",
    "        \"\"\"Get the occurence of a given column value from the hash table\n",
    "    \n",
    "        column_name -- Column in hash table (\"feature name\")\n",
    "        value -- Value that we want to check the occurence for\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.__hash_table[column_name].loc[value]\n",
    "        except KeyError:\n",
    "            # Value not found, so the occurence is zero\n",
    "            return 0\n",
    "\n",
    "    def get_hash_table(self, column_name):\n",
    "        \"\"\"Returns the hash table for the column name\"\"\"\n",
    "        return self.__hash_table[column_name]\n",
    "\n",
    "    def get_subfeature_hash_table(self, column_name, subfeature):\n",
    "        \"\"\"Returns the hash table for the column name\"\"\"\n",
    "        return self.__hash_table[(column_name, subfeature)]\n",
    "\n",
    "    def len_unique(self, column_name):\n",
    "        \"\"\"Return the number of unique values in hash table column\n",
    "        \n",
    "        column_name -- Column in hash table (\"feature name\")\n",
    "        \"\"\"\n",
    "        return len(self.__hash_table[column_name])\n",
    "\n",
    "    def len(self):\n",
    "        \"\"\"Return the total number of values per hash table column\n",
    "        \"\"\"\n",
    "        return self.__length\n",
    "        \n",
    "    def increase(self, log):\n",
    "        \"\"\"Increase the hash table values based on the log values\n",
    "        \n",
    "        log -- New log entry\n",
    "        \"\"\"\n",
    "        # For each value\n",
    "        for key, value in log._asdict().items():\n",
    "            # Add value only when key is in the hash table\n",
    "            if key in self.__hash_table:\n",
    "                try:\n",
    "                    # Increase value\n",
    "                    self.__hash_table[key].loc[value] = self.__hash_table[key].loc[value] + 1\n",
    "                except KeyError:\n",
    "                    #  Key does not exist yet, so we start with 1\n",
    "                    self.__hash_table[key].loc[value] = 1\n",
    "\n",
    "                    # Sort index\n",
    "                    self.__hash_table[key].sort_index(inplace=True)\n",
    "\n",
    "                # TODO: Increase subfeature values\n",
    "                if key in self.__subfeatures.keys():\n",
    "                    for subfeature in self.__subfeatures[key]:\n",
    "                        # Create subfeature hash tables\n",
    "                        self.get_subfeature_hash_table(key, subfeature).increase(log)\n",
    "                \n",
    "        # Increase length of hash table\n",
    "        self.__length = self.__length + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubfeatureOccurenceHashTableHelper:\n",
    "    \"\"\"Helper class to adjust functions to RBA algorithm function calls\"\"\"\n",
    "\n",
    "    def __init__(self, hash_table, column_name, subfeature, subfeature_value, login_attempt):\n",
    "        \"\"\"Constructor\n",
    "        \n",
    "        subfeature_occurence_hash_table -- SubfeatureOccurenceHashTable\n",
    "        subfeature_value -- subfeature value that we should adjust the functions to\n",
    "        \"\"\"\n",
    "        # Global hash table\n",
    "        self.__hash_table = hash_table\n",
    "\n",
    "        # Subfeature hash table\n",
    "        self.__subfeature = subfeature\n",
    "        self.__subfeature_occurence_hash_table = self.__hash_table.get_subfeature_hash_table(column_name, self.__subfeature)\n",
    "        self.__subfeature_value = subfeature_value\n",
    "\n",
    "        # Current login attempt\n",
    "        self.__login_attempt = login_attempt\n",
    "\n",
    "    def len(self):\n",
    "        #print(self.__subfeature_value)\n",
    "        # Filter by subfeature value\n",
    "        return self.get(self.__subfeature, self.__subfeature_value)\n",
    "\n",
    "    def get(self, feature, feature_value):\n",
    "        #raise NotImplementedError(\"Function get() is not implemented at the moment.\")\n",
    "        # global_hash_table.get(\"ipASN\", 2119.0)\n",
    "        return self.__hash_table.get(feature, feature_value)\n",
    "\n",
    "    def len_unique(self, next_subfeature):\n",
    "        # Reproduce subfeature values from prior subfeatures\n",
    "\n",
    "        # Get list of subfeatures\n",
    "        subfeatures_to_use = self.get_subfeature_occurence_hash_table().get_subfeatures()\n",
    "\n",
    "        # Filter the list to only use the features before the next subfeature\n",
    "        subfeatures_to_use = subfeatures_to_use[:subfeatures_to_use.index(next_subfeature)]\n",
    "        \n",
    "        # Build the list of values\n",
    "        values = []\n",
    "\n",
    "        # Iterate through this list\n",
    "        for subfeature in subfeatures_to_use:\n",
    "            # Append the feature value for current feature \n",
    "            values.append(getattr(self.__login_attempt, subfeature))\n",
    "        \n",
    "        # Convert to tuple to that we can index in hash table\n",
    "        values = tuple(values)\n",
    "\n",
    "        # Nested subfeature values\n",
    "        try:\n",
    "            return len(self.get_subfeature_occurence_hash_table().get_full_subfeature_table().loc[values])\n",
    "        except KeyError as e:\n",
    "            if e != \"nan\":\n",
    "                # Subfeature value does not exist in table\n",
    "                return 0.0\n",
    "\n",
    "    def get_hash_table(self):\n",
    "        return self.__subfeature_occurence_hash_table.get_hash_table(self.__subfeature_value)\n",
    "\n",
    "    def get_subfeature_occurence_hash_table(self):\n",
    "        return self.__subfeature_occurence_hash_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk-Based Authentication (RBA) Algorithm\n",
    "\n",
    "This is our RBA implementation following the instructions given in [Freeman et al. (2016)](https://doi.org/10.14722/ndss.2016.23240). The original implementation was never published. Therefore, we cannot be sure that it is identical with the one in Freeman et al., especially with the edge cases that we discovered and addressed here.\n",
    "\n",
    "The code below was used in Wiefling et al. ([2022](https://doi.org/10.1145/3546069)) and was performance optimized from the version in Wiefling et al. ([2021a](https://pub.h-brs.de/files/5956/Wiefling2021_WhatsInScoreForWebsiteUsers-Postproceedings.pdf), [2021b](https://nbn-resolving.org/urn:nbn:de:hbz:1044-opus-58417)). See the publications for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Weightings for the risk score calculation. These values are set for\n",
    "our RBA data set, which you can obtain at\n",
    "https://github.com/das-group/rba-dataset\n",
    "\"\"\"\n",
    "feature_weightings = {\n",
    "    \"requestXRealIP\": {\n",
    "        \"requestXRealIP\": 0.6,\n",
    "        \"ipASN\": 0.3,\n",
    "        \"ipCountry\": 0.1\n",
    "    },\n",
    "    \"trackingdatauserAgent\": {\n",
    "        'trackingdatauserAgent': 0.5386653840551359,\n",
    "        'browser_name_version': 0.2680451498625666,\n",
    "        'os_name_version': 0.18818295100109536,\n",
    "        'device_type': 0.0051065150812021525\n",
    "    },\n",
    "}\n",
    "\n",
    "risk_score_weightings = {\n",
    "    'requestXRealIP': 1,\n",
    "    'trackingdatauserAgent': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unseen_values(login_history, feature, hash_table=None):\n",
    "    # Look for posible subfeatures\n",
    "    for __feature in feature_weightings.keys():\n",
    "        # Iterate through all possible subfeature names\n",
    "        i=0\n",
    "        for subfeature in feature_weightings[__feature].keys():\n",
    "            # Exit this loop when we reached the end of the iteration\n",
    "            if i > len(feature_weightings[__feature].keys())-2:\n",
    "                break\n",
    "            \n",
    "            # When we found the feature that we were looking for\n",
    "            if feature == subfeature:\n",
    "                # Get the next subfeature to jump to\n",
    "                next_subfeature = list(feature_weightings[__feature].keys())[i+1]\n",
    "                \n",
    "                # Do recursion\n",
    "                try:\n",
    "                    return hash_table.len_unique(next_subfeature) + get_unseen_values(login_history, next_subfeature, hash_table)\n",
    "                except (AttributeError, NotImplementedError) as e:\n",
    "                    # Hash table or hash table function does not exist, so we use the slower algorithm instead\n",
    "                    return len(login_history[next_subfeature].unique()) + get_unseen_values(login_history, next_subfeature, hash_table)\n",
    "            i+=1\n",
    "        \n",
    "    # No subfeatures found. So we only assume one unseen value.\n",
    "    return 1\n",
    "\n",
    "def get_likelihood(login_history, login_attempt, feature, smoothing=True, hash_table=None):\n",
    "    # Get the feature value of login attempt\n",
    "    feature_value = getattr(login_attempt, feature)\n",
    "\n",
    "    try:\n",
    "        appearance = hash_table.get(feature, feature_value)\n",
    "    except (AttributeError, NotImplementedError) as e:\n",
    "        # Hash table or hash table function does not exist, so we use the slower algorithm instead\n",
    "        appearance = len(login_history.query(\"{} == @feature_value\".format(feature)))\n",
    "    \n",
    "    if smoothing == True or appearance == 0:\n",
    "        unseen = get_unseen_values(login_history, feature, hash_table)\n",
    "    else:\n",
    "        unseen = 0\n",
    "\n",
    "    try:\n",
    "        num_logs = hash_table.len()\n",
    "    except (AttributeError, NotImplementedError) as e:\n",
    "        num_logs = len(login_history)    \n",
    "\n",
    "    if appearance > 0:\n",
    "        likelihood = appearance/num_logs\n",
    "        likelihood *= (1 - (unseen/(num_logs+unseen)))\n",
    "    else:\n",
    "        if smoothing == True:\n",
    "            likelihood = 1/(num_logs+unseen)\n",
    "        else:\n",
    "            # Feature does not appear at all and is not smoothed\n",
    "            likelihood = 0.0\n",
    "        \n",
    "    return likelihood\n",
    "\n",
    "def get_sub_likelihood(user_logs, login_attempt, feature, feature_value, subfeature, smoothing=True, hash_table=None):\n",
    "    subfeature_value = getattr(login_attempt, subfeature)\n",
    "    \n",
    "    try:\n",
    "        hash_table_subfeature = SubfeatureOccurenceHashTableHelper(hash_table, feature, subfeature, subfeature_value, login_attempt)\n",
    "    except:\n",
    "        hash_table_subfeature = None\n",
    "\n",
    "    if hash_table_subfeature == None:\n",
    "        user_logs_subfeature = user_logs.query(\"{} == @subfeature_value\".format(subfeature))\n",
    "    else:\n",
    "        user_logs_subfeature = None\n",
    "\n",
    "    likelihood_user_subfeature = get_likelihood(user_logs_subfeature, login_attempt, subfeature, smoothing=smoothing, hash_table=hash_table_subfeature)\n",
    "    likelihood_subfeature = get_likelihood(user_logs, login_attempt, subfeature, smoothing=smoothing, hash_table=hash_table)\n",
    "\n",
    "    return likelihood_user_subfeature*likelihood_subfeature\n",
    "\n",
    "def get_minimum_likelihood(login_history, feature, smoothing=True, hash_table=None):\n",
    "    \"\"\"Returns the minimum possible likelihood for a given feature\"\"\"\n",
    "    num_logs = len(login_history)\n",
    "    \n",
    "    if smoothing == True:\n",
    "        unseen = get_unseen_values(login_history, feature, hash_table)\n",
    "    else:\n",
    "        unseen = 0\n",
    "    \n",
    "    return 1/(num_logs+unseen)\n",
    "\n",
    "def get_minimum_sub_likelihood(login_history, feature, subfeature, hash_table=None):\n",
    "    \"\"\"Returns the minimum possible likelihood for a given subfeature\"\"\"\n",
    "    # Calculate likelihood that feature is of subfeature\n",
    "    likelihood_user_subfeature = get_minimum_likelihood(login_history, subfeature, smoothing=False, hash_table=hash_table)\n",
    "\n",
    "    # Calculate likelihood of subfeature\n",
    "    likelihood_subfeature = get_minimum_likelihood(login_history, subfeature, hash_table=hash_table)\n",
    "\n",
    "    return likelihood_user_subfeature*likelihood_subfeature\n",
    "\n",
    "def freeman_rba_score(login_attempt, user_logs, global_logs, num_users, features=[\"requestXRealIP\", \"trackingdatauserAgent\"], global_hash_table=None):\n",
    "    \"\"\"Return risk score based on Freeman et al.'s model\n",
    "    (Equation 7, without per-member attack data)\n",
    "    \n",
    "    Keyword arguments:\n",
    "    login_attempt -- Pandas DataFrame containing current login attempt\n",
    "    user_logs -- Pandas DataFrame containing past login sessions of user\n",
    "    global_logs -- Pandas DataFrame containing all login sessions of all users\n",
    "    num_users -- number of users in the global login history\n",
    "    features -- Python list of features to be evaluated\n",
    "    global_hash_table -- Hash table containing the global occurences per feature and feature value (generated by create_occurence_hash_table(...))\n",
    "\n",
    "    return -- Risk score\"\"\"\n",
    "\n",
    "    if isinstance(login_attempt, pd.Series):\n",
    "        print(\"ERROR: login_attempt is a pandas Series. This breaks the functionality of the hash table. Abort.\")\n",
    "        return\n",
    "        \n",
    "    # Probability of how likely this feature value is used by an attacker\n",
    "    # We don't have any data here, so we don't weigh it here\n",
    "    feature_value_attack_probability = 1\n",
    "    \n",
    "    \n",
    "    # Get ID of user\n",
    "    userid = user_logs.head(1)[\"userid\"].squeeze()\n",
    "            \n",
    "    risk_score = 1.0\n",
    "    # Iterate through each feature\n",
    "    for feature in features:    \n",
    "        # Get the feature value of login attempt\n",
    "        feature_value = getattr(login_attempt, feature)\n",
    "        \n",
    "        # Calculate how likely this feature value appears in user's login history\n",
    "        local_unseen = get_unseen_values(user_logs, feature)\n",
    "        num_user_logs = len(user_logs)\n",
    "\n",
    "        local_likelihood = get_likelihood(user_logs, login_attempt, feature)\n",
    "        minimum_likelihood = get_minimum_likelihood(user_logs, feature)\n",
    "        \n",
    "        # If we have subsets in a feature, we process them\n",
    "        if feature in feature_weightings:            \n",
    "            local_likelihood = 0.0\n",
    "            minimum_likelihood = 0.0\n",
    "\n",
    "            # Level in feature hierarchy\n",
    "            smoothing = False\n",
    "            for subfeature in feature_weightings[feature]:\n",
    "                subfeature_value = getattr(login_attempt, subfeature)\n",
    "                \n",
    "                __local_likelihood = feature_weightings[feature][subfeature] * get_sub_likelihood(user_logs, login_attempt, \"requestXRealIP\", feature_value, subfeature, smoothing=smoothing)\n",
    "                local_likelihood += math.pow(__local_likelihood, risk_score_weightings[feature])\n",
    "                \n",
    "                __minimum_likelihood = feature_weightings[feature][subfeature] * get_minimum_sub_likelihood(user_logs, feature, subfeature)\n",
    "                minimum_likelihood += math.pow(__minimum_likelihood, risk_score_weightings[feature])\n",
    "                \n",
    "                # We only smooth the first element in hierarchy\n",
    "                smoothing = False\n",
    "        \n",
    "        # Calculate how likely this feature value appears in global login history\n",
    "        if global_hash_table != None:\n",
    "            num_global_logs = global_hash_table.len()\n",
    "        else:\n",
    "            num_global_logs = len(global_logs)\n",
    "\n",
    "        global_likelihood = get_likelihood(global_logs, login_attempt, feature, hash_table=global_hash_table)\n",
    "\n",
    "        # In cases where we have subsets of features\n",
    "        if feature in feature_weightings:            \n",
    "            global_likelihood = 0.0\n",
    "            smoothing = True\n",
    "            for subfeature in feature_weightings[feature]:\n",
    "                sub_likelihood = get_sub_likelihood(global_logs, login_attempt, feature, feature_value, subfeature, smoothing=smoothing, hash_table=global_hash_table)\n",
    "                \n",
    "                __global_likelihood = feature_weightings[feature][subfeature] * sub_likelihood\n",
    "                global_likelihood += math.pow(__global_likelihood, risk_score_weightings[feature])\n",
    "                \n",
    "                # We only smooth the first element in hierarchy\n",
    "                smoothing = False\n",
    "                           \n",
    "        if local_likelihood == 0.0:\n",
    "            \"\"\"\n",
    "            Edge case: The feature value never appeared in the user's login history.\n",
    "            Smoothing this value, however, would result in a low risk score when local\n",
    "            and global value both have to be smoothed. So we make sure that the risk\n",
    "            score for this edge case is high.\n",
    "            \"\"\"\n",
    "            local_likelihood = global_likelihood / 4\n",
    "                \n",
    "        # Calculate part of risk score for feature\n",
    "        risk_score *= global_likelihood/local_likelihood\n",
    "        \n",
    "    # Likelihood of user logging in\n",
    "    user_login_likelihood = num_user_logs/num_global_logs\n",
    "\n",
    "    # We assume that all users are to be attacked equally\n",
    "    attack_likelihood = 1/num_users\n",
    "    \n",
    "    risk_score = risk_score * (attack_likelihood/user_login_likelihood)\n",
    "    \n",
    "    return risk_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example using RBA data set\n",
    "\n",
    "We show an example test case using our RBA data set, which you can obtain at https://github.com/das-group/rba-dataset\n",
    "\n",
    "Extract the data set's ZIP file contents to a `rba-dataset` folder inside the folder of this Jupyter Notebook to run the example code.\n",
    "\n",
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users:  271719\n",
      "Sessions:  476564\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 476564 entries, 2020-02-03 12:43:55.873000 to 2020-02-16 16:49:41.146000\n",
      "Data columns (total 9 columns):\n",
      " #   Column                 Non-Null Count   Dtype \n",
      "---  ------                 --------------   ----- \n",
      " 0   userid                 476564 non-null  int64 \n",
      " 1   requestXRealIP         476564 non-null  object\n",
      " 2   ipCountry              476564 non-null  object\n",
      " 3   ipASN                  476564 non-null  int64 \n",
      " 4   trackingdatauserAgent  476564 non-null  object\n",
      " 5   browser_name_version   476564 non-null  object\n",
      " 6   os_name_version        476564 non-null  object\n",
      " 7   device_type            476564 non-null  object\n",
      " 8   Login Successful       476564 non-null  bool  \n",
      "dtypes: bool(1), int64(2), object(6)\n",
      "memory usage: 33.2+ MB\n"
     ]
    }
   ],
   "source": [
    "def rename_columns(logs):\n",
    "    \"\"\"\n",
    "    Rename columns to match the names inside the algorithm.\n",
    "    \"\"\"\n",
    "    logs.rename(columns={\n",
    "        \"User ID\": \"userid\",\n",
    "        \"IP Address\": \"requestXRealIP\",\n",
    "        \"ASN\": \"ipASN\",\n",
    "        \"Country\": \"ipCountry\",\n",
    "        \"User Agent String\": \"trackingdatauserAgent\",\n",
    "        \"Browser Name and Version\": \"browser_name_version\",\n",
    "        \"OS Name and Version\": \"os_name_version\",\n",
    "        \"Device Type\": \"device_type\"\n",
    "    }, inplace=True)\n",
    "\n",
    "__columns_to_use = [\n",
    "    \"Login Timestamp\", \n",
    "    \"User ID\",\n",
    "    \"IP Address\", \n",
    "    \"Country\", \n",
    "    \"ASN\", \n",
    "    \"User Agent String\", \n",
    "    \"Browser Name and Version\", \n",
    "    \"OS Name and Version\", \n",
    "    \"Device Type\",\n",
    "    \"Login Successful\"\n",
    "]\n",
    "\n",
    "global_logs = pd.read_csv(\n",
    "    \"./rba-dataset/rba-dataset.csv\", \n",
    "    index_col=\"Login Timestamp\", \n",
    "    parse_dates=True, \n",
    "    usecols=__columns_to_use,\n",
    "    nrows=1000000\n",
    ")\n",
    "\n",
    "# Drop all invalid values\n",
    "global_logs.dropna(inplace=True)\n",
    "\n",
    "# We only consider successful login attempts for our risk score calculation\n",
    "global_logs = global_logs[global_logs['Login Successful'] == True]\n",
    "rename_columns(global_logs)\n",
    "\n",
    "# Sort the values by timestamp to fasten accessing the values (binary search)\n",
    "global_logs.sort_index(inplace=True)\n",
    "\n",
    "# Get all userids\n",
    "user_ids = global_logs[\"userid\"].unique()\n",
    "\n",
    "print(\"Users: \", len(user_ids))\n",
    "print(\"Sessions: \", len(global_logs))\n",
    "print()\n",
    "global_logs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** As you can see in the data columns listing above, many columns are of `object` Dtype. You can further improve the performance by hashing the values of these columns and store them as `int64`. See [Wiefling et al. (2022)](https://doi.org/10.1145/3546069) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test risk scores for legitimate logins\n",
    "### Define test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group global logs by userid to save time with each iteration\n",
    "global_logs_groupby_userid = global_logs.groupby(\"userid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login_test_single(slice_start, slice_size, features=[\"requestXRealIP\", \"trackingdatauserAgent\"]):\n",
    "    \"\"\"Login test (risk scores for legitimate logins): single execution\n",
    "    Feel free to include this function in a Python multiprocessing \n",
    "    environment to unleash multi-core calculation power.\n",
    "\n",
    "    slice_start -- Start position of the slice\n",
    "    slice_size -- Size of the slice\n",
    "    \"\"\"\n",
    "\n",
    "    # Create hash table at start of calculation\n",
    "\n",
    "    # We have to extract the global login history before the starting point of our login test\n",
    "    __logs = global_logs.iloc[:slice_start]\n",
    "\n",
    "    # Get log entries until end of slice size\n",
    "    __login_attempts = global_logs.iloc[slice_start:slice_start+slice_size]\n",
    "\n",
    "    # Create hash table\n",
    "    global_hash_table = OccurenceHashTable(__logs, __logs.columns)\n",
    "    \n",
    "    # Table containing the risk score results\n",
    "    risk_score_results = nested_dict()\n",
    "\n",
    "    # Iterate for each login attempt\n",
    "    i = 1\n",
    "\n",
    "    for login_attempt in __login_attempts.itertuples():\n",
    "        # Extract log for userid\n",
    "        userid = getattr(login_attempt, \"userid\")\n",
    "\n",
    "        # Extract timestamp\n",
    "        timestamp = getattr(login_attempt, \"Index\")\n",
    "                \n",
    "        # Get logs for user\n",
    "        user_logs = global_logs_groupby_userid.get_group(userid).before_timestamp(timestamp)\n",
    "\n",
    "        # Login attempt number\n",
    "        login_attempt_number = len(user_logs) + 1\n",
    "\n",
    "        # Only execute when we have more than one login attempt for user\n",
    "        if login_attempt_number > 1:\n",
    "            # Get number of users from hash table\n",
    "            num_users = global_hash_table.len_unique(\"userid\")\n",
    "\n",
    "            # Execute RBA\n",
    "            risk_score = freeman_rba_score(login_attempt, user_logs, global_logs, num_users, features=features, global_hash_table=global_hash_table)\n",
    "            \n",
    "            # Add risk score to results\n",
    "            risk_score_results[\"userid\"].append(userid)\n",
    "            risk_score_results[\"sessionStart\"].append(timestamp)\n",
    "            risk_score_results[\"login_attempt_number\"].append(login_attempt_number)\n",
    "            risk_score_results[\"risk_score\"].append(risk_score)\n",
    "\n",
    "        # Add login attempt to hash table and increase the feature value counts here\n",
    "        global_hash_table.increase(login_attempt)\n",
    "\n",
    "        # Increase the log entries to include the current login attempt\n",
    "        __logs = global_logs.head(slice_start+i)\n",
    "\n",
    "        # Increase login attempt counter\n",
    "        i+=1\n",
    "\n",
    "    return pd.DataFrame(risk_score_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the test\n",
    "\n",
    "We calculate the risk scores for the data set entries 50001-50051 and check whether the calculated risk scores are as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>sessionStart</th>\n",
       "      <th>login_attempt_number</th>\n",
       "      <th>risk_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-5325525697941282628</td>\n",
       "      <td>2020-02-04 17:47:54.110</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4053139217885951811</td>\n",
       "      <td>2020-02-04 17:47:55.633</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1925965161544074057</td>\n",
       "      <td>2020-02-04 17:47:55.662</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7232833366797706854</td>\n",
       "      <td>2020-02-04 17:47:56.298</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9022591335621324689</td>\n",
       "      <td>2020-02-04 17:47:58.560</td>\n",
       "      <td>2</td>\n",
       "      <td>0.006298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3697714251381343037</td>\n",
       "      <td>2020-02-04 17:48:01.492</td>\n",
       "      <td>3</td>\n",
       "      <td>0.024024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6849543229299398287</td>\n",
       "      <td>2020-02-04 17:48:01.648</td>\n",
       "      <td>2</td>\n",
       "      <td>0.362419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-2691043712332620674</td>\n",
       "      <td>2020-02-04 17:48:05.887</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1732494684414432077</td>\n",
       "      <td>2020-02-04 17:48:10.613</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-8293692664622708457</td>\n",
       "      <td>2020-02-04 17:48:25.289</td>\n",
       "      <td>3</td>\n",
       "      <td>1.246223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4235518659815716095</td>\n",
       "      <td>2020-02-04 17:48:26.575</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5251410046017156379</td>\n",
       "      <td>2020-02-04 17:48:31.327</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4568506544548498954</td>\n",
       "      <td>2020-02-04 17:48:36.331</td>\n",
       "      <td>2</td>\n",
       "      <td>0.470148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4903391907790429539</td>\n",
       "      <td>2020-02-04 17:48:38.860</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7595515139309677557</td>\n",
       "      <td>2020-02-04 17:48:46.618</td>\n",
       "      <td>2</td>\n",
       "      <td>0.084123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1581972485609172348</td>\n",
       "      <td>2020-02-04 17:48:50.648</td>\n",
       "      <td>3</td>\n",
       "      <td>0.442128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-8338960771738518323</td>\n",
       "      <td>2020-02-04 17:48:51.955</td>\n",
       "      <td>2</td>\n",
       "      <td>0.014721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2108256618707178903</td>\n",
       "      <td>2020-02-04 17:48:55.749</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4469624674141872550</td>\n",
       "      <td>2020-02-04 17:48:55.890</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1925965161544074057</td>\n",
       "      <td>2020-02-04 17:48:57.435</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4847753662475335057</td>\n",
       "      <td>2020-02-04 17:48:58.408</td>\n",
       "      <td>2</td>\n",
       "      <td>0.004581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1651207708054818202</td>\n",
       "      <td>2020-02-04 17:48:59.461</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2696734826840910700</td>\n",
       "      <td>2020-02-04 17:49:07.753</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-8261561586125610729</td>\n",
       "      <td>2020-02-04 17:49:08.880</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 userid            sessionStart  login_attempt_number  \\\n",
       "0  -5325525697941282628 2020-02-04 17:47:54.110                     9   \n",
       "1   4053139217885951811 2020-02-04 17:47:55.633                     4   \n",
       "2  -1925965161544074057 2020-02-04 17:47:55.662                     3   \n",
       "3   7232833366797706854 2020-02-04 17:47:56.298                     2   \n",
       "4  -9022591335621324689 2020-02-04 17:47:58.560                     2   \n",
       "5   3697714251381343037 2020-02-04 17:48:01.492                     3   \n",
       "6   6849543229299398287 2020-02-04 17:48:01.648                     2   \n",
       "7  -2691043712332620674 2020-02-04 17:48:05.887                     3   \n",
       "8   1732494684414432077 2020-02-04 17:48:10.613                     2   \n",
       "9  -8293692664622708457 2020-02-04 17:48:25.289                     3   \n",
       "10  4235518659815716095 2020-02-04 17:48:26.575                     2   \n",
       "11  5251410046017156379 2020-02-04 17:48:31.327                     4   \n",
       "12  4568506544548498954 2020-02-04 17:48:36.331                     2   \n",
       "13  4903391907790429539 2020-02-04 17:48:38.860                     3   \n",
       "14  7595515139309677557 2020-02-04 17:48:46.618                     2   \n",
       "15 -1581972485609172348 2020-02-04 17:48:50.648                     3   \n",
       "16 -8338960771738518323 2020-02-04 17:48:51.955                     2   \n",
       "17  2108256618707178903 2020-02-04 17:48:55.749                     2   \n",
       "18  4469624674141872550 2020-02-04 17:48:55.890                     3   \n",
       "19 -1925965161544074057 2020-02-04 17:48:57.435                     4   \n",
       "20  4847753662475335057 2020-02-04 17:48:58.408                     2   \n",
       "21  1651207708054818202 2020-02-04 17:48:59.461                     8   \n",
       "22  2696734826840910700 2020-02-04 17:49:07.753                     2   \n",
       "23 -8261561586125610729 2020-02-04 17:49:08.880                     6   \n",
       "\n",
       "    risk_score  \n",
       "0     0.001736  \n",
       "1     0.000313  \n",
       "2     0.000385  \n",
       "3     0.000234  \n",
       "4     0.006298  \n",
       "5     0.024024  \n",
       "6     0.362419  \n",
       "7     0.000349  \n",
       "8     0.000487  \n",
       "9     1.246223  \n",
       "10    0.000260  \n",
       "11    0.000220  \n",
       "12    0.470148  \n",
       "13    0.000980  \n",
       "14    0.084123  \n",
       "15    0.442128  \n",
       "16    0.014721  \n",
       "17    0.033660  \n",
       "18    0.000484  \n",
       "19    0.000257  \n",
       "20    0.004581  \n",
       "21    0.000012  \n",
       "22    0.000654  \n",
       "23    0.000905  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test success: Risk scores are as expected\n"
     ]
    }
   ],
   "source": [
    "test_result = login_test_single(50000, 50)\n",
    "display(test_result)\n",
    "\n",
    "expected_result = [\n",
    "    0.001735608896331267,\n",
    "    0.0003131930911696338,\n",
    "    0.00038481209981864445,\n",
    "    0.00023380343388881817,\n",
    "    0.0062984271682455266,\n",
    "    0.024024171011906934,\n",
    "    0.36241860205465537,\n",
    "    0.0003489615152250577,\n",
    "    0.00048701529441907065,\n",
    "    1.2462230273339967,\n",
    "    0.00025999322274249494,\n",
    "    0.00021976714969851479,\n",
    "    0.47014766381586093,\n",
    "    0.0009798412309385818,\n",
    "    0.0841229474444737,\n",
    "    0.4421278704947991,\n",
    "    0.01472119251257646,\n",
    "    0.03366013914910071,\n",
    "    0.000483684972634349,\n",
    "    0.0002568913432172573,\n",
    "    0.0045807146185027755,\n",
    "    1.176522284255606e-05,\n",
    "    0.0006543352945740962,\n",
    "    0.0009051562075623795\n",
    "]\n",
    "\n",
    "if test_result[\"risk_score\"].to_list() == expected_result:\n",
    "    print(\"Test success: Risk scores are as expected\")\n",
    "else:\n",
    "    print(\"Test failed: Risk scores are not as expected\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
